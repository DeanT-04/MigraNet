# PubMed_Gephi_Refined.py - ç²¾ç‚¼ç‰ˆPubMedç½‘ç»œæ„å»ºå™¨
# ä¸“é—¨è§£å†³èŠ‚ç‚¹è¿‡å¤šã€åˆ†ç±»ä¸ç²¾ç¡®çš„é—®é¢˜

import pandas as pd
import re
import ast
from collections import defaultdict, Counter
import itertools
import os


class PubMedRefinedNetwork:
    def __init__(self):
        # ä¸¥æ ¼çš„åŒ»å­¦åœç”¨è¯ï¼ˆå¤§å¹…æ‰©å±•ï¼‰
        self.medical_stopwords = {
            "study",
            "studies",
            "research",
            "analysis",
            "effect",
            "effects",
            "patient",
            "patients",
            "group",
            "groups",
            "method",
            "methods",
            "result",
            "results",
            "conclusion",
            "conclusions",
            "objective",
            "background",
            "aim",
            "purpose",
            "significance",
            "review",
            "article",
            "paper",
            "their",
            "with",
            "the",
            "and",
            "or",
            "for",
            "from",
            "this",
            "that",
            "these",
            "those",
            "which",
            "what",
            "when",
            "where",
            "how",
            "why",
            "has",
            "have",
            "had",
            "was",
            "were",
            "is",
            "are",
            "be",
            "been",
            "being",
            "can",
            "could",
            "would",
            "should",
            "may",
            "might",
            "must",
            "author",
            "theory",
            "model",
            "system",
            "process",
            "approach",
            "perspective",
            "overview",
            "summary",
            "current",
            "future",
            "recent",
            "new",
            "novel",
            "various",
        }

        # åŸºäºæ‚¨è®ºæ–‡æ¡†æ¶çš„ç²¾ç¡®å®šä¹‰åˆ†ç±»ä½“ç³»
        self.refined_categories = {
            # 1. è¯±å‘æœºåˆ¶ï¼ˆä¸¥æ ¼é™å®šï¼‰
            "trigger_mechanisms": {
                "description": "è¯±å‘æœºåˆ¶",
                "keywords": [
                    # ç¥ç»æœºåˆ¶
                    "trigeminal",
                    "trigeminovascular",
                    "cortical spreading depression",
                    "central sensitization",
                    "neurogenic inflammation",
                    "neural mechanism",
                    # è¡€ç®¡æœºåˆ¶
                    "vascular",
                    "cerebral blood flow",
                    "vasodilation",
                    "vasoconstriction",
                    # æ¿€ç´ æœºåˆ¶
                    "hormonal",
                    "estrogen",
                    "progesterone",
                    "menstrual",
                    "menopause",
                    # ç‚ç—‡æœºåˆ¶
                    "inflammatory",
                    "cytokines",
                    "neuroinflammation",
                    "mast cells",
                    # ç¯å¢ƒè§¦å‘
                    "stress",
                    "sleep deprivation",
                    "weather",
                    "barometric",
                    "light sensitivity",
                ],
            },
            # 2. ç¡®è¯å…±ç—…ï¼ˆä¸¥æ ¼åŒ»å­¦å®šä¹‰ï¼‰
            "true_comorbidities": {
                "description": "ç¡®è¯å…±ç—…",
                "keywords": [
                    # ç²¾ç¥ç±»
                    "depression",
                    "anxiety",
                    "panic disorder",
                    "bipolar",
                    "ptsd",
                    # ç¥ç»ç±»
                    "epilepsy",
                    "stroke",
                    "restless legs",
                    "parkinson",
                    "alzheimer",
                    # ç–¼ç—›ç±»
                    "fibromyalgia",
                    "chronic pain",
                    "neuropathic pain",
                    # è‡ªèº«å…ç–«/è¿‡æ•
                    "allergic rhinitis",
                    "asthma",
                    "irritable bowel",
                    "inflammatory bowel",
                    # ç¡çœ éšœç¢
                    "insomnia",
                    "sleep apnea",
                    "circadian rhythm",
                    # å¿ƒè¡€ç®¡
                    "hypertension",
                    "patent foramen ovale",
                    "stroke risk",
                ],
            },
            # 3. ç¤¾ä¼šå½±å“
            "social_impact": {
                "description": "ç¤¾ä¼šå½±å“",
                "keywords": [
                    "quality of life",
                    "disability",
                    "work productivity",
                    "absenteeism",
                    "presenteeism",
                    "economic burden",
                    "healthcare cost",
                    "stigma",
                    "social isolation",
                    "family burden",
                    "daily functioning",
                ],
            },
            # 4. å¹²é¢„æªæ–½
            "interventions": {
                "description": "å¹²é¢„æªæ–½",
                "keywords": [
                    # è¯ç‰©
                    "triptans",
                    "CGRP",
                    "erenumab",
                    "fremanezumab",
                    "galcanezumab",
                    "propranolol",
                    "topiramate",
                    "amitriptyline",
                    "valproate",
                    "botulinum",
                    # éè¯ç‰©
                    "cognitive behavioral therapy",
                    "biofeedback",
                    "acupuncture",
                    "physical therapy",
                    "relaxation",
                    "mindfulness",
                    "yoga",
                    # ç”Ÿæ´»æ–¹å¼
                    "diet",
                    "exercise",
                    "sleep hygiene",
                    "stress management",
                    # æ–°å…´æ²»ç–—
                    "neuromodulation",
                    "monoclonal antibodies",
                    "gene therapy",
                ],
            },
        }

        # éœ€è¦æ’é™¤çš„ç ”ç©¶æ–¹æ³•æœ¯è¯­ï¼ˆå•ç‹¬åˆ†ç±»ï¼‰
        self.research_methods = {
            "randomized controlled trial",
            "cohort study",
            "case control",
            "cross sectional",
            "systematic review",
            "meta analysis",
            "clinical trial",
            "observational study",
            "diagnostic criteria",
            "assessment scale",
            "statistical analysis",
            "epidemiology",
        }

    def load_pubmed_data(self, file_path):
        """åŠ è½½æ•°æ®"""
        try:
            # å°è¯•å¤šç§æ ¼å¼
            for encoding in ["utf-8", "latin-1", "cp1252"]:
                for sep in [",", "\t", ";"]:
                    try:
                        df = pd.read_csv(
                            file_path, encoding=encoding, sep=sep, quoting=1, on_bad_lines="skip"
                        )
                        if len(df.columns) > 3:
                            print(f"æˆåŠŸè¯»å–: {encoding}, åˆ†éš”ç¬¦: '{sep}'")
                            return df
                    except:
                        continue
            return pd.read_csv(file_path, engine="python", on_bad_lines="skip")
        except Exception as e:
            print(f"è¯»å–å¤±è´¥: {e}")
            return None

    def strict_term_cleaning(self, term):
        """ä¸¥æ ¼çš„æœ¯è¯­æ¸…ç†"""
        if pd.isna(term) or not term.strip():
            return None

        term = str(term).strip()

        # 1. å»é™¤æ‰€æœ‰ç‰¹æ®Šæ ‡è®°
        term = re.sub(r"^\*+|\*+$", "", term)  # å¼€å¤´ç»“å°¾æ˜Ÿå·
        term = re.sub(r"/\*.*", "", term)  # æ–œæ æ˜Ÿå·åå†…å®¹
        term = re.sub(r"\[.*?\]|\(.*?\)", "", term)  # æ‹¬å·å†…å®¹

        # 2. è½¬æ¢ä¸ºå°å†™å¹¶åˆ†å‰²
        term = term.lower()
        words = term.split()

        # 3. ä¸¥æ ¼è¿‡æ»¤
        filtered_words = []
        for word in words:
            # é•¿åº¦è¦æ±‚
            if len(word) < 3 or len(word) > 20:
                continue
            # åœç”¨è¯è¿‡æ»¤
            if word in self.medical_stopwords:
                continue
            # æ•°å­—è¿‡æ»¤
            if word.isdigit():
                continue
            # ç‰¹æ®Šå­—ç¬¦æ£€æŸ¥
            if re.search(r"[^a-z\-]", word):
                continue

            filtered_words.append(word)

        if not filtered_words:
            return None

        term = " ".join(filtered_words)

        # 4. æ ‡é¢˜åŒ–å¹¶è¿”å›
        return term.title()

    def precise_categorization(self, term):
        """ç²¾ç¡®åˆ†ç±»"""
        term_lower = term.lower()

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯ç ”ç©¶æ–¹æ³•ï¼ˆéœ€è¦æ’é™¤ï¼‰
        for research_term in self.research_methods:
            if research_term in term_lower:
                return "research_methods"  # å•ç‹¬æ ‡è®°ï¼Œåç»­è¿‡æ»¤

        # ç²¾ç¡®åŒ¹é…åˆ†ç±»
        for category, info in self.refined_categories.items():
            for keyword in info["keywords"]:
                if keyword in term_lower:
                    return category

        # åŸºäºå†…å®¹æ¨æ–­ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        trigger_words = ["mechanism", "pathophysiology", "etiology", "trigger", "sensitization"]
        comorbidity_words = ["comorbidity", "comorbid", "coexisting", "associated with"]
        impact_words = ["burden", "cost", "productivity", "quality", "disability"]
        intervention_words = ["therapy", "treatment", "medication", "management", "intervention"]

        if any(word in term_lower for word in trigger_words):
            return "trigger_mechanisms"
        elif any(word in term_lower for word in comorbidity_words):
            return "true_comorbidities"
        elif any(word in term_lower for word in impact_words):
            return "social_impact"
        elif any(word in term_lower for word in intervention_words):
            return "interventions"

        return "unclassified"  # æœªåˆ†ç±»çš„å°†è¢«è¿‡æ»¤

    def extract_high_quality_terms(self, tags_str, content_text=""):
        """æå–é«˜è´¨é‡æœ¯è¯­"""
        if pd.isna(tags_str):
            return []

        tags_str = str(tags_str)
        high_quality_terms = set()

        # åˆ†å‰²æ ‡ç­¾
        segments = re.split(r"[;,]", tags_str)

        for segment in segments:
            cleaned_term = self.strict_term_cleaning(segment)
            if not cleaned_term:
                continue

            # åˆ†ç±»æ£€æŸ¥
            category = self.precise_categorization(cleaned_term)

            # åªä¿ç•™æ˜ç¡®åˆ†ç±»çš„æœ¯è¯­
            if category != "unclassified" and category != "research_methods":
                high_quality_terms.add(cleaned_term)

        return list(high_quality_terms)

    def build_refined_network(self, df, min_frequency=3, min_weight=2):
        """æ„å»ºç²¾ç‚¼ç½‘ç»œ"""
        print("æ„å»ºç²¾ç‚¼ç½‘ç»œï¼ˆå¤§å¹…å‡å°‘èŠ‚ç‚¹æ•°é‡ï¼‰...")

        article_keywords = []

        for idx, row in df.iterrows():
            if idx % 200 == 0 and idx > 0:
                print(f"å¤„ç†è¿›åº¦: {idx}/{len(df)}")

            # æå–é«˜è´¨é‡æœ¯è¯­
            manual_tags = self.extract_high_quality_terms(row.get("Manual Tags", ""))

            # é™åˆ¶æ¯ç¯‡æ–‡ç« çš„æœ¯è¯­æ•°é‡ï¼ˆé˜²æ­¢å•ç¯‡æ–‡ç« è´¡çŒ®è¿‡å¤šèŠ‚ç‚¹ï¼‰
            if len(manual_tags) > 15:
                manual_tags = manual_tags[:15]

            if manual_tags:
                article_keywords.append(manual_tags)

        print(f"æœ‰æ•ˆæ–‡ç« æ•°: {len(article_keywords)}")

        # è®¡ç®—èŠ‚ç‚¹é¢‘ç‡
        node_frequency = Counter()
        for keywords in article_keywords:
            node_frequency.update(keywords)

        # ä¸¥æ ¼è¿‡æ»¤ï¼šåªä¿ç•™é«˜é¢‘æœ¯è¯­
        filtered_terms = {
            term: freq for term, freq in node_frequency.items() if freq >= min_frequency
        }

        print(f"è¿‡æ»¤åæœ¯è¯­æ•°: {len(filtered_terms)} (åŸ: {len(node_frequency)})")

        # è®¡ç®—è¾¹æƒé‡ï¼ˆåªè€ƒè™‘è¿‡æ»¤åçš„æœ¯è¯­ï¼‰
        edge_weights = defaultdict(int)
        for keywords in article_keywords:
            # åªè€ƒè™‘è¿‡æ»¤åçš„æœ¯è¯­
            filtered_keywords = [kw for kw in keywords if kw in filtered_terms]
            for kw1, kw2 in itertools.combinations(sorted(filtered_keywords), 2):
                edge_weights[(kw1, kw2)] += 1

        # åˆ›å»ºèŠ‚ç‚¹æ•°æ®
        nodes_data = []
        for term, freq in filtered_terms.items():
            category = self.precise_categorization(term)
            nodes_data.append(
                {
                    "Id": re.sub(r"[^\w]", "_", term.lower())[:30],
                    "Label": term,
                    "Category": category,
                    "Frequency": freq,
                    "Category_Description": self.refined_categories.get(category, {}).get(
                        "description", "å…¶ä»–"
                    ),
                }
            )

        # åˆ›å»ºè¾¹æ•°æ®ï¼ˆåº”ç”¨æƒé‡é˜ˆå€¼ï¼‰
        edges_data = []
        for (term1, term2), weight in edge_weights.items():
            if weight >= min_weight:  # æƒé‡é˜ˆå€¼
                source_id = next(
                    (node["Id"] for node in nodes_data if node["Label"] == term1), None
                )
                target_id = next(
                    (node["Id"] for node in nodes_data if node["Label"] == term2), None
                )

                if source_id and target_id:
                    edges_data.append(
                        {
                            "Source": source_id,
                            "Target": target_id,
                            "Weight": weight,
                            "Type": "Undirected",
                            "Source_Label": term1,
                            "Target_Label": term2,
                        }
                    )

        nodes_df = pd.DataFrame(nodes_data)
        edges_df = pd.DataFrame(edges_data)

        print(f"æœ€ç»ˆç½‘ç»œè§„æ¨¡: {len(nodes_df)}èŠ‚ç‚¹, {len(edges_df)}è¾¹")
        print(f"èŠ‚ç‚¹å‡å°‘: {(3645 - len(nodes_df)) / 3645 * 100:.1f}%")
        print(f"è¾¹å‡å°‘: {(181245 - len(edges_df)) / 181245 * 100:.1f}%")

        return nodes_df, edges_df

    def analyze_refined_network(self, nodes_df, edges_df):
        """åˆ†æç²¾ç‚¼ç½‘ç»œ"""
        print("\n" + "=" * 60)
        print("ç²¾ç‚¼ç½‘ç»œåˆ†ææŠ¥å‘Š")
        print("=" * 60)

        # åŸºæœ¬ç»Ÿè®¡
        print(f"ç½‘ç»œè§„æ¨¡:")
        print(f"  - èŠ‚ç‚¹æ•°: {len(nodes_df):,} (åŸ3,645ä¸ª)")
        print(f"  - è¾¹æ•°: {len(edges_df):,} (åŸ181,245æ¡)")
        print(f"  - ç½‘ç»œå¯†åº¦: {len(edges_df) / (len(nodes_df) * (len(nodes_df) - 1) / 2):.6f}")

        # ç±»åˆ«åˆ†å¸ƒ
        print(f"\nèŠ‚ç‚¹åˆ†ç±»åˆ†å¸ƒ:")
        category_stats = nodes_df["Category"].value_counts()
        for category, count in category_stats.items():
            desc = self.refined_categories.get(category, {}).get("description", "å…¶ä»–")
            percentage = (count / len(nodes_df)) * 100
            print(f"  - {desc}: {count}èŠ‚ç‚¹ ({percentage:.1f}%)")

        # é«˜é¢‘æœ¯è¯­
        print(f"\nTop 20 é«˜é¢‘æœ¯è¯­:")
        top_terms = nodes_df.nlargest(20, "Frequency")
        for idx, row in top_terms.iterrows():
            desc = self.refined_categories.get(row["Category"], {}).get("description", "å…¶ä»–")
            print(f"  {idx + 1:2d}. {row['Label']:25s} (é¢‘ç‡: {row['Frequency']:2d}, ç±»åˆ«: {desc})")

        # å¼ºå…³è”
        if not edges_df.empty:
            print(f"\nTop 10 å¼ºå…±ç°å…³ç³»:")
            top_edges = edges_df.nlargest(10, "Weight")
            for idx, row in top_edges.iterrows():
                print(
                    f"  {idx + 1:2d}. {row['Source_Label']:20s} â†â†’ {row['Target_Label']:20s} (æƒé‡: {row['Weight']})"
                )


def main():
    """ä¸»å‡½æ•°"""
    converter = PubMedRefinedNetwork()

    # æ–‡ä»¶è·¯å¾„
    file_path = r"C:\Users\29385\Desktop\å¤§ä¸‰\å¤æ‚ç½‘ç»œ\åå¤´ç—›2\PubMed.csv"

    if not os.path.exists(file_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    # åŠ è½½æ•°æ®
    print("åŠ è½½PubMedæ•°æ®...")
    df = converter.load_pubmed_data(file_path)

    if df is None or df.empty:
        print("æ•°æ®åŠ è½½å¤±è´¥")
        return

    print(f"æ•°æ®è§„æ¨¡: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")

    # æ„å»ºç²¾ç‚¼ç½‘ç»œï¼ˆæé«˜é˜ˆå€¼ï¼‰
    nodes_df, edges_df = converter.build_refined_network(df, min_frequency=3, min_weight=2)

    if nodes_df.empty:
        print("ç½‘ç»œæ„å»ºå¤±è´¥")
        return

    # åˆ†æç½‘ç»œ
    converter.analyze_refined_network(nodes_df, edges_df)

    # ä¿å­˜æ–‡ä»¶
    output_dir = os.path.dirname(file_path)

    # Gephiæ–‡ä»¶
    nodes_df[["Id", "Label", "Category", "Frequency"]].to_csv(
        os.path.join(output_dir, "gephi_refined_nodes.csv"), index=False, encoding="utf-8-sig"
    )

    edges_df[["Source", "Target", "Weight", "Type"]].to_csv(
        os.path.join(output_dir, "gephi_refined_edges.csv"), index=False, encoding="utf-8-sig"
    )

    # è¯¦ç»†æ–‡ä»¶ï¼ˆç”¨äºåˆ†æï¼‰
    nodes_df.to_csv(
        os.path.join(output_dir, "detailed_refined_nodes.csv"), index=False, encoding="utf-8-sig"
    )
    edges_df.to_csv(
        os.path.join(output_dir, "detailed_refined_edges.csv"), index=False, encoding="utf-8-sig"
    )

    print(f"\nâœ… ç²¾ç‚¼ç½‘ç»œæ–‡ä»¶å·²ç”Ÿæˆï¼")
    print(f"ğŸ“Š é¢„æœŸç½‘ç»œè§„æ¨¡: {len(nodes_df)}èŠ‚ç‚¹, {len(edges_df)}è¾¹")
    print(
        f"ğŸ“‰ è§„æ¨¡å‡å°‘: èŠ‚ç‚¹-{(3645 - len(nodes_df)) / 3645 * 100:.1f}%, è¾¹-{(181245 - len(edges_df)) / 181245 * 100:.1f}%"
    )

    # Gephiä¼˜åŒ–å‚æ•°
    gephi_guide = """
    ğŸ¯ Gephiä¼˜åŒ–å‚æ•°ï¼ˆé’ˆå¯¹ç²¾ç‚¼ç½‘ç»œï¼‰:

    å¸ƒå±€ç®—æ³•: ForceAtlas 2
    - æ–¥åŠ›å¼ºåº¦: 2000 (åŸ1000)
    - é‡åŠ›: 50 (åŸ5) 
    - é˜²æ­¢é‡å : âœ… å¼€å¯
    - è¾¹æƒé‡å½±å“: 1.0
    - è¿è¡Œæ—¶é—´: 3-5åˆ†é’Ÿ

    å¤–è§‚è®¾ç½®:
    - èŠ‚ç‚¹é¢œè‰²: æŒ‰Categoryå­—æ®µ
    - èŠ‚ç‚¹å¤§å°: æŒ‰Frequencyå­—æ®µ (2-15èŒƒå›´)
    - æ ‡ç­¾: åªæ˜¾ç¤ºFrequency > 5çš„èŠ‚ç‚¹
    - è¾¹é€æ˜åº¦: 0.3 (æé«˜å¯è¯»æ€§)
    """

    print(gephi_guide)

    # ä¿å­˜æŒ‡å—
    with open(os.path.join(output_dir, "gephi_optimized_guide.txt"), "w", encoding="utf-8") as f:
        f.write(gephi_guide)


if __name__ == "__main__":
    main()
